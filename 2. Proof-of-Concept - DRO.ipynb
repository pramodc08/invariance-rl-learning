{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a167bcb2-85c5-4d7d-bec4-18cbe12212e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07af07b-4429-4346-b1b8-d94b812e2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED, num_threads=2):\n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    torch.set_num_threads(num_threads)\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(num_threads)\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = str(num_threads)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4730538e-65af-4a28-a9c7-fd36dfd216b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lunar_lander import LunarLander\n",
    "from wrapper import NonCausalInternalLunarLanderObsWrapper\n",
    "base_env = LunarLander(enable_wind=False)\n",
    "env_1 = NonCausalInternalLunarLanderObsWrapper(base_env, scale=1.0, noise_level=0.01)\n",
    "env_2 = NonCausalInternalLunarLanderObsWrapper(base_env, scale=1.0, noise_level=0.01)\n",
    "env_3 = NonCausalInternalLunarLanderObsWrapper(base_env, scale=1.0, noise_level=0.01)\n",
    "envs = [env_1, env_2, env_3]\n",
    "for i_env in envs:\n",
    "    i_env.reset(seed=SEED)\n",
    "test_env_1 = NonCausalInternalLunarLanderObsWrapper(base_env, scale=0.0, noise_level=0.0)  # No noise\n",
    "test_env_2 = NonCausalInternalLunarLanderObsWrapper(base_env, scale=1.0, noise_level=0.0) # Within range but not trained\n",
    "test_env_3 = NonCausalInternalLunarLanderObsWrapper(base_env, scale=1.0, noise_level=0.0)  # Out of range\n",
    "test_envs = [test_env_1, test_env_2, test_env_3]\n",
    "for i_env in test_envs:\n",
    "    i_env.reset(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da81959f-5cc8-4ead-ac51-1ad3ea957e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "state_shape = envs[0].observation_space.shape[0]\n",
    "action_shape = envs[0].action_space.n\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "BUFFER_SIZE = 50000  # Replay memory size\n",
    "BATCH_SIZE = 64       # Minibatch size\n",
    "GAMMA = 0.99          # Discount factor\n",
    "TAU = 1e-3            # Soft update of target parameters\n",
    "LR = 1e-4             # Learning rate\n",
    "UPDATE_EVERY = 5      # How often to update the network\n",
    "N_EPOCHS = 5      \n",
    "# Training parameters\n",
    "n_episodes = 3000\n",
    "max_t = 1000\n",
    "eps_start = 1.0\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.995\n",
    "recurrent = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5675a-55ec-490a-986d-d4e93785d33c",
   "metadata": {},
   "source": [
    "## Shared buffer single agent using step method from agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99152d40-97d4-4122-81bc-b755cee61ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Vanilla DQN Agent (MLP)...\n",
      "DeepQNetwork(\n",
      "  (fcs): ModuleList(\n",
      "    (0): Linear(in_features=19, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  )\n",
      "  (norms): ModuleList(\n",
      "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (output): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "\n",
    "agent = Agent(state_size=state_shape, action_size=action_shape, device=device,\n",
    "        recurrent=recurrent, lr=LR, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA,\n",
    "        tau=TAU, update_every=UPDATE_EVERY, n_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f3d080-9344-474d-8714-8174517ada04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Episode 100\tEnv0: -107.29 | Env1: -117.98 | Env2: -114.04\tEps: 0.606\n",
      "------------------------------------------------------------------------------------------\n",
      "Episode 100\n",
      "  Env0: Train Avg(100)=-107.29 | Eval=-241.66\n",
      "  Env1: Train Avg(100)=-117.98 | Eval=-304.19\n",
      "  Env2: Train Avg(100)=-114.04 | Eval=-193.72\n",
      "  TestEnv0: Test Eval=-125.31\n",
      "  TestEnv1: Test Eval=-281.38\n",
      "  TestEnv2: Test Eval=-231.69\n",
      "Episode 200\tEnv0: -83.11 | Env1: -83.56 | Env2: -84.64\tEps: 0.367711\n",
      "------------------------------------------------------------------------------------------\n",
      "Episode 200\n",
      "  Env0: Train Avg(100)=-83.11 | Eval=64.60\n",
      "  Env1: Train Avg(100)=-83.56 | Eval=63.72\n",
      "  Env2: Train Avg(100)=-84.64 | Eval=104.55\n",
      "  TestEnv0: Test Eval=-152.67\n",
      "  TestEnv1: Test Eval=32.31\n",
      "  TestEnv2: Test Eval=-22.49\n",
      "Episode 256\tEnv0: -47.23 | Env1: -58.44 | Env2: -53.81\tEps: 0.277"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     29\u001b[0m terminal \u001b[38;5;241m=\u001b[39m done \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m---> 30\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhiddens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_hidden\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m states[i] \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     36\u001b[0m hiddens[i] \u001b[38;5;241m=\u001b[39m next_hidden\n",
      "File \u001b[0;32m~/pramod_private/invariance-rl-learning/agent.py:73\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, state, action, reward, next_state, done, hidden, next_hidden)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m     72\u001b[0m     experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pramod_private/invariance-rl-learning/agent.py:137\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m    135\u001b[0m     Q_expected \u001b[38;5;241m=\u001b[39m current_out\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     Q_expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnetwork_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    140\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(Q_expected, Q_targets)\n",
      "File \u001b[0;32m~/pramod_private/invariance-rl-learning/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pramod_private/invariance-rl-learning/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/pramod_private/invariance-rl-learning/model.py:66\u001b[0m, in \u001b[0;36mDeepQNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, norm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms):\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(x)  \u001b[38;5;66;03m# Switched to GELU\u001b[39;00m\n",
      "File \u001b[0;32m~/pramod_private/invariance-rl-learning/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_envs = len(envs)\n",
    "n_test_envs = len(test_envs)\n",
    "\n",
    "train_returns = [[] for _ in range(n_envs)]\n",
    "scores_window = [deque(maxlen=100) for _ in range(n_envs)]\n",
    "train_mean_returns = [[] for _ in range(n_envs)]  # avg value during training rollout\n",
    "\n",
    "env_eval_returns  = [[] for _ in range(n_envs)]\n",
    "test_eval_returns = [[] for _ in range(n_test_envs)]\n",
    "\n",
    "eps = eps_start\n",
    "print(\"Training started...\")\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    states = []\n",
    "    hiddens = []\n",
    "    dones = [False] * n_envs\n",
    "    episode_scores = np.zeros(n_envs, dtype=np.float32)\n",
    "    for env in envs:\n",
    "        s, _ = env.reset()\n",
    "        states.append(s)\n",
    "        hiddens.append(agent.qnetwork_local.init_hidden(1))\n",
    "    # --- rollout ---\n",
    "    for t in range(max_t):\n",
    "        for i, env in enumerate(envs):\n",
    "            if dones[i]:\n",
    "                continue\n",
    "            action, next_hidden = agent.act(states[i], eps, hiddens[i])\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            terminal = done or truncated\n",
    "            agent.step(\n",
    "                states[i], action, reward,\n",
    "                next_state, terminal,\n",
    "                hiddens[i], next_hidden\n",
    "            )\n",
    "            states[i] = next_state\n",
    "            hiddens[i] = next_hidden\n",
    "            episode_scores[i] += reward\n",
    "            dones[i] = terminal\n",
    "        if all(dones):\n",
    "            break\n",
    "\n",
    "    for i in range(n_envs):\n",
    "        ep_ret = float(episode_scores[i])\n",
    "        train_returns[i].append(ep_ret)\n",
    "        scores_window[i].append(ep_ret)\n",
    "        train_mean_returns[i].append(np.mean(scores_window[i]))\n",
    "\n",
    "    eps = max(eps_end, eps_decay * eps)\n",
    "    env_stats = \" | \".join(\n",
    "        f\"Env{i}: {np.mean(scores_window[i]):.2f}\"\n",
    "        for i in range(n_envs)\n",
    "    )\n",
    "    print(\n",
    "        f\"\\rEpisode {i_episode}\"\n",
    "        f\"\\t{env_stats}\"\n",
    "        f\"\\tEps: {eps:.3f}\",\n",
    "        end=\"\"\n",
    "    )\n",
    "\n",
    "    if i_episode % 10 == 0:\n",
    "        for i, env in enumerate(envs):\n",
    "            eval_list = []\n",
    "            for _ in range(5):\n",
    "                state, _ = env.reset()\n",
    "                hidden = agent.qnetwork_local.init_hidden(1)\n",
    "                total = 0.0\n",
    "\n",
    "                for t in range(max_t):\n",
    "                    action, next_hidden = agent.act(state, eps=0.0, hidden=hidden)\n",
    "                    next_state, reward, done, truncated, _ = env.step(action)\n",
    "                    state = next_state\n",
    "                    hidden = next_hidden\n",
    "                    total += reward\n",
    "                    if done or truncated:\n",
    "                        break\n",
    "                eval_list.append(total)\n",
    "            env_eval_returns[i].append(float(np.mean(eval_list)))\n",
    "             \n",
    "        for i, env in enumerate(test_envs):\n",
    "            eval_list = []\n",
    "            for _ in range(5):\n",
    "                state, _ = env.reset()\n",
    "                hidden = agent.qnetwork_local.init_hidden(1)\n",
    "                total = 0.0\n",
    "                for t in range(max_t):\n",
    "                    action, next_hidden = agent.act(\n",
    "                        state, eps=0.0, hidden=hidden\n",
    "                    )\n",
    "                    next_state, reward, done, truncated, _ = env.step(action)\n",
    "                    state = next_state\n",
    "                    hidden = next_hidden\n",
    "                    total += reward\n",
    "                    if done or truncated:\n",
    "                        break\n",
    "                eval_list.append(total)\n",
    "            test_eval_returns[i].append(np.mean(eval_list))\n",
    "    # --- clean newline every 100 eps ---\n",
    "    if i_episode % 100 == 0:\n",
    "        print(\"\\n\" + \"-\" * 90)\n",
    "        print(f\"Episode {i_episode}\")\n",
    "        for i in range(n_envs):\n",
    "            line = f\"  Env{i}: Train Avg(100)={np.mean(scores_window[i]):.2f}\"\n",
    "            if env_eval_returns[i]:\n",
    "                line += f\" | Eval={env_eval_returns[i][-1]:.2f}\"\n",
    "            print(line)\n",
    "\n",
    "        for i in range(n_test_envs):\n",
    "            print(f\"  TestEnv{i}: Test Eval={test_eval_returns[i][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c8daa-4574-424b-ac09-b7f27e1cb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = len(envs)\n",
    "n_test_envs = len(test_envs)\n",
    "\n",
    "eval_every = 10  # you evaluate every 10 episodes\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "# --- Training scores ---\n",
    "for i in range(n_envs):\n",
    "    x = np.arange(1, len(train_mean_returns[i]) + 1)\n",
    "    ax1.plot(x, train_mean_returns[i], label=f'Env-{i}')\n",
    "    \n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_xlabel('Episode #')\n",
    "ax1.set_title('Training Returns (Averaged)')\n",
    "ax1.grid(True)\n",
    "ax1.legend(ncol=1)\n",
    "\n",
    "for i in range(n_envs):\n",
    "    eval_x = np.arange(1, len(env_eval_returns[i]) + 1) * eval_every\n",
    "    ax2.plot(\n",
    "        eval_x,\n",
    "        env_eval_returns[i],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        label=f'Env-{i}'\n",
    "    )\n",
    "\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_xlabel('Episode #')\n",
    "ax2.set_title(f'Evaluation on training envs (eps=0, every {eval_every} episodes)')\n",
    "ax2.grid(True)\n",
    "ax2.legend(ncol=1)\n",
    "\n",
    "for i in range(n_test_envs):\n",
    "    eval_test_x = np.arange(1, len(test_eval_returns[i]) + 1) * eval_every\n",
    "    ax3.plot(\n",
    "        eval_test_x,\n",
    "        test_eval_returns[i],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        label=f'Test-Env-{i}'\n",
    "    )\n",
    "\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_xlabel('Episode #')\n",
    "ax3.set_title(f'Evaluation on test envs (eps=0, every {eval_every} episodes)')\n",
    "ax3.grid(True)\n",
    "ax3.legend(ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a4e7a-9205-4f4a-b362-d2d42190be2a",
   "metadata": {},
   "source": [
    "## Individual buffer single agent using step method from agent\n",
    "\n",
    " For mean optimization with pure RL Based algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc87e8a-cd2e-488a-86a6-ae36581bf4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffers import ReplayBuffer, HiddenStateReplayBuffer\n",
    "from agent import Agent\n",
    "\n",
    "buffers = [\n",
    "    HiddenStateReplayBuffer(\n",
    "        action_size=action_shape,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device\n",
    "    ) if recurrent else ReplayBuffer(\n",
    "        action_size=action_shape,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device\n",
    "    )\n",
    "    for _ in envs\n",
    "]\n",
    "group_weights = torch.tensor([0.5 for i in envs]) # Start with equal weighting for env_1 and env_2\n",
    "\n",
    "dro_agent = Agent(state_size=state_shape, action_size=action_shape, device=device,\n",
    "        recurrent=recurrent, lr=LR, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA,\n",
    "        tau=TAU, update_every=UPDATE_EVERY, n_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95732ddd-f891-4e94-9a08-e1dcaafed46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(agent, experiences):\n",
    "    \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    if agent.recurrent:\n",
    "        states, actions, rewards, next_states, dones, hidden, next_hidden = experiences\n",
    "    else:\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "    # ------------------- Compute Target Q ------------------- #\n",
    "    with torch.no_grad():\n",
    "        if agent.recurrent:\n",
    "            # Double DQN for LSTM:\n",
    "            # 1. Select best action using Local Net + Next Hidden State\n",
    "            local_next_out, _ = agent.qnetwork_local(next_states, next_hidden)\n",
    "            best_actions = local_next_out.argmax(1).unsqueeze(1)\n",
    "            \n",
    "            # 2. Evaluate using Target Net + Next Hidden State\n",
    "            target_next_out, _ = agent.qnetwork_target(next_states, next_hidden)\n",
    "            Q_targets_next = target_next_out.gather(1, best_actions)\n",
    "        else:\n",
    "            # Double DQN for MLP:\n",
    "            best_actions = agent.qnetwork_local(next_states).argmax(1).unsqueeze(1)\n",
    "            Q_targets_next = agent.qnetwork_target(next_states).gather(1, best_actions)\n",
    "        Q_targets = rewards + (agent.gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "\n",
    "    if agent.recurrent:\n",
    "        # Forward pass with the STORED hidden state from the buffer\n",
    "        current_out, _ = agent.qnetwork_local(states, hidden)\n",
    "        Q_expected = current_out.gather(1, actions)\n",
    "    else:\n",
    "        Q_expected = agent.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.smooth_l1_loss(Q_expected, Q_targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede9624-d0ab-4075-981e-00cbf5032082",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = len(envs)\n",
    "n_test_envs = len(test_envs)\n",
    "\n",
    "train_returns = [[] for _ in range(n_envs)]\n",
    "scores_window = [deque(maxlen=100) for _ in range(n_envs)]\n",
    "train_mean_returns = [[] for _ in range(n_envs)]  # avg value during training rollout\n",
    "\n",
    "env_eval_returns  = [[] for _ in range(n_envs)]\n",
    "test_eval_returns = [[] for _ in range(n_test_envs)]\n",
    "\n",
    "eps = eps_start\n",
    "t_step = 0\n",
    "print(\"Training started...\")\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    states = []\n",
    "    hiddens = []\n",
    "    dones = [False] * n_envs\n",
    "    episode_scores = np.zeros(n_envs, dtype=np.float32)\n",
    "    for env in envs:\n",
    "        s, _ = env.reset()\n",
    "        states.append(s)\n",
    "        hiddens.append(dro_agent.qnetwork_local.init_hidden(1))\n",
    "    # --- rollout ---\n",
    "    for t in range(max_t):\n",
    "        for i, env in enumerate(envs):\n",
    "            if dones[i]:\n",
    "                continue\n",
    "            action, next_hidden = dro_agent.act(states[i], eps, hiddens[i])\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            terminal = done or truncated\n",
    "\n",
    "            if recurrent:\n",
    "                buffers[i].add(states[i], action, reward, next_state, terminal, hiddens[i], next_hidden)\n",
    "            else:\n",
    "                buffers[i].add(states[i], action, reward, next_state, terminal)\n",
    "\n",
    "            states[i] = next_state\n",
    "            hiddens[i] = next_hidden\n",
    "            episode_scores[i] += reward\n",
    "            dones[i] = terminal\n",
    "        \n",
    "        # Train step\n",
    "        t_step += 1\n",
    "        if all([len(i) > dro_agent.batch_size for i in buffers]):\n",
    "            t_step = (t_step + 1) % dro_agent.update_every\n",
    "            if t_step == 0:\n",
    "                for _ in range(len(envs)):\n",
    "                    losses = []\n",
    "                    for i, env in enumerate(envs):\n",
    "                        experience = buffers[i].sample()\n",
    "                        loss_i = learn(dro_agent, experience)\n",
    "                        losses.append(loss_i)\n",
    "    \n",
    "                    loss = torch.stack(losses).mean()\n",
    "                    dro_agent.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(dro_agent.qnetwork_local.parameters(), 1.0)\n",
    "                    dro_agent.optimizer.step()\n",
    "                    \n",
    "                    dro_agent.soft_update(dro_agent.qnetwork_local, dro_agent.qnetwork_target, dro_agent.tau)\n",
    "\n",
    "        if all(dones):\n",
    "            break\n",
    "        \n",
    "    for i in range(n_envs):\n",
    "        ep_ret = float(episode_scores[i])\n",
    "        train_returns[i].append(ep_ret)\n",
    "        scores_window[i].append(ep_ret)\n",
    "        train_mean_returns[i].append(np.mean(scores_window[i]))\n",
    "\n",
    "    eps = max(eps_end, eps_decay * eps)\n",
    "    env_stats = \" | \".join(\n",
    "        f\"Env{i}: {np.mean(scores_window[i]):.2f}\"\n",
    "        for i in range(n_envs)\n",
    "    )\n",
    "    print(\n",
    "        f\"\\rEpisode {i_episode}\"\n",
    "        f\"\\t{env_stats}\"\n",
    "        f\"\\tEps: {eps:.3f}\",\n",
    "        end=\"\"\n",
    "    )\n",
    "\n",
    "    if i_episode % 10 == 0:\n",
    "        for i, env in enumerate(envs):\n",
    "            eval_list = []\n",
    "            for _ in range(5):\n",
    "                state, _ = env.reset()\n",
    "                hidden = dro_agent.qnetwork_local.init_hidden(1)\n",
    "                total = 0.0\n",
    "\n",
    "                for t in range(max_t):\n",
    "                    action, next_hidden = dro_agent.act(state, eps=0.0, hidden=hidden)\n",
    "                    next_state, reward, done, truncated, _ = env.step(action)\n",
    "                    state = next_state\n",
    "                    hidden = next_hidden\n",
    "                    total += reward\n",
    "                    if done or truncated:\n",
    "                        break\n",
    "                eval_list.append(total)\n",
    "            env_eval_returns[i].append(float(np.mean(eval_list)))\n",
    "            \n",
    "        for i, env in enumerate(test_envs):\n",
    "            eval_list = []\n",
    "            for _ in range(5):\n",
    "                state, _ = env.reset()\n",
    "                hidden = dro_agent.qnetwork_local.init_hidden(1)\n",
    "                total = 0\n",
    "                for t in range(max_t):\n",
    "                    action, next_hidden = dro_agent.act(\n",
    "                        state, eps=0.0, hidden=hidden\n",
    "                    )\n",
    "                    next_state, reward, done, truncated, _ = env.step(action)\n",
    "                    state = next_state\n",
    "                    hidden = next_hidden\n",
    "                    total += reward\n",
    "                    if done or truncated:\n",
    "                        break\n",
    "                eval_list.append(total)\n",
    "            test_eval_returns[i].append(np.mean(eval_list))\n",
    "    # --- clean newline every 100 eps ---\n",
    "    if i_episode % 100 == 0:\n",
    "        print(\"\\n\" + \"-\" * 90)\n",
    "        print(f\"Episode {i_episode}\")\n",
    "        for i in range(n_envs):\n",
    "            line = f\"  Env{i}: Train Avg(100)={np.mean(scores_window[i]):.2f}\"\n",
    "            if env_eval_returns[i]:\n",
    "                line += f\" | Eval={env_eval_returns[i][-1]:.2f}\"\n",
    "            print(line)\n",
    "\n",
    "        for i in range(n_test_envs):\n",
    "            print(f\"  TestEnv{i}: Test Eval={test_eval_returns[i][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b36e57-a93f-4a3e-86e3-0952494e5971",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = len(envs)\n",
    "n_test_envs = len(test_envs)\n",
    "\n",
    "eval_every = 10  # you evaluate every 10 episodes\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "# --- Training scores ---\n",
    "for i in range(n_envs):\n",
    "    x = np.arange(1, len(train_mean_returns[i]) + 1)\n",
    "    ax1.plot(x, train_mean_returns[i], label=f'Env-{i}')\n",
    "    \n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_xlabel('Episode #')\n",
    "ax1.set_title('Training Returns (Averaged)')\n",
    "ax1.grid(True)\n",
    "ax1.legend(ncol=1)\n",
    "\n",
    "for i in range(n_envs):\n",
    "    eval_x = np.arange(1, len(env_eval_returns[i]) + 1) * eval_every\n",
    "    ax2.plot(\n",
    "        eval_x,\n",
    "        env_eval_returns[i],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        label=f'Env-{i}'\n",
    "    )\n",
    "\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_xlabel('Episode #')\n",
    "ax2.set_title(f'Evaluation on training envs (eps=0, every {eval_every} episodes)')\n",
    "ax2.grid(True)\n",
    "ax2.legend(ncol=1)\n",
    "\n",
    "for i in range(n_test_envs):\n",
    "    eval_test_x = np.arange(1, len(test_eval_returns[i]) + 1) * eval_every\n",
    "    ax3.plot(\n",
    "        eval_test_x,\n",
    "        test_eval_returns[i],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        label=f'Test-Env-{i}'\n",
    "    )\n",
    "\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_xlabel('Episode #')\n",
    "ax3.set_title(f'Evaluation on test envs (eps=0, every {eval_every} episodes)')\n",
    "ax3.grid(True)\n",
    "ax3.legend(ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d0f69-d6ac-41bb-9171-be470c266bf2",
   "metadata": {},
   "source": [
    "## Individual buffer single agent using step method from agent with DRO implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2977d44-4107-4381-9862-e96102c28a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffers import ReplayBuffer, HiddenStateReplayBuffer\n",
    "from agent import Agent\n",
    "\n",
    "buffers = [\n",
    "    HiddenStateReplayBuffer(\n",
    "        action_size=action_shape,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device\n",
    "    ) if recurrent else ReplayBuffer(\n",
    "        action_size=action_shape,\n",
    "        buffer_size=BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device\n",
    "    )\n",
    "    for _ in envs\n",
    "]\n",
    "group_weights = torch.tensor([0.5 for i in envs]) # Start with equal weighting for env_1 and env_2\n",
    "\n",
    "dro_agent = Agent(state_size=state_shape, action_size=action_shape, device=device,\n",
    "        recurrent=recurrent, lr=LR, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA,\n",
    "        tau=TAU, update_every=UPDATE_EVERY, n_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfe73c0-df60-4cc9-8cc0-819f27a261db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(agent, experiences):\n",
    "    \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "    if agent.recurrent:\n",
    "        states, actions, rewards, next_states, dones, hidden, next_hidden = experiences\n",
    "    else:\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "    # ------------------- Compute Target Q ------------------- #\n",
    "    with torch.no_grad():\n",
    "        if agent.recurrent:\n",
    "            # Double DQN for LSTM:\n",
    "            # 1. Select best action using Local Net + Next Hidden State\n",
    "            local_next_out, _ = agent.qnetwork_local(next_states, next_hidden)\n",
    "            best_actions = local_next_out.argmax(1).unsqueeze(1)\n",
    "            \n",
    "            # 2. Evaluate using Target Net + Next Hidden State\n",
    "            target_next_out, _ = agent.qnetwork_target(next_states, next_hidden)\n",
    "            Q_targets_next = target_next_out.gather(1, best_actions)\n",
    "        else:\n",
    "            # Double DQN for MLP:\n",
    "            best_actions = agent.qnetwork_local(next_states).argmax(1).unsqueeze(1)\n",
    "            Q_targets_next = agent.qnetwork_target(next_states).gather(1, best_actions)\n",
    "        Q_targets = rewards + (agent.gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "\n",
    "    if agent.recurrent:\n",
    "        # Forward pass with the STORED hidden state from the buffer\n",
    "        current_out, _ = agent.qnetwork_local(states, hidden)\n",
    "        Q_expected = current_out.gather(1, actions)\n",
    "    else:\n",
    "        Q_expected = agent.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.smooth_l1_loss(Q_expected, Q_targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511aae5-45cc-4f08-98ae-20dd15fde908",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = len(envs)\n",
    "n_test_envs = len(test_envs)\n",
    "\n",
    "train_returns = [[] for _ in range(n_envs)]\n",
    "scores_window = [deque(maxlen=100) for _ in range(n_envs)]\n",
    "train_mean_returns = [[] for _ in range(n_envs)]  # avg value during training rollout\n",
    "\n",
    "env_eval_returns  = [[] for _ in range(n_envs)]\n",
    "test_eval_returns = [[] for _ in range(n_test_envs)]\n",
    "\n",
    "eps = eps_start\n",
    "t_step = 0\n",
    "print(\"Training started...\")\n",
    "for i_episode in range(1, n_episodes + 1):\n",
    "    states = []\n",
    "    hiddens = []\n",
    "    dones = [False] * n_envs\n",
    "    episode_scores = np.zeros(n_envs, dtype=np.float32)\n",
    "    for env in envs:\n",
    "        s, _ = env.reset()\n",
    "        states.append(s)\n",
    "        hiddens.append(dro_agent.qnetwork_local.init_hidden(1))\n",
    "    # --- rollout ---\n",
    "    for t in range(max_t):\n",
    "        for i, env in enumerate(envs):\n",
    "            if dones[i]:\n",
    "                continue\n",
    "            action, next_hidden = dro_agent.act(states[i], eps, hiddens[i])\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            terminal = done or truncated\n",
    "\n",
    "            if recurrent:\n",
    "                buffers[i].add(states[i], action, reward, next_state, terminal, hiddens[i], next_hidden)\n",
    "            else:\n",
    "                buffers[i].add(states[i], action, reward, next_state, terminal)\n",
    "\n",
    "            states[i] = next_state\n",
    "            hiddens[i] = next_hidden\n",
    "            episode_scores[i] += reward\n",
    "            dones[i] = terminal\n",
    "        \n",
    "        # Train step\n",
    "        t_step += 1\n",
    "        if all([len(i) > dro_agent.batch_size for i in buffers]):\n",
    "            t_step = (t_step + 1) % dro_agent.update_every\n",
    "            if t_step == 0:\n",
    "                for _ in range(len(envs)):\n",
    "                    losses = []\n",
    "                    for i, env in enumerate(envs):\n",
    "                        experience = buffers[i].sample()\n",
    "                        loss_i = learn(dro_agent, experience)\n",
    "                        losses.append(loss_i)\n",
    "    \n",
    "                    loss = max(losses)\n",
    "                    dro_agent.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(dro_agent.qnetwork_local.parameters(), 1.0)\n",
    "                    dro_agent.optimizer.step()\n",
    "                    \n",
    "                    dro_agent.soft_update(dro_agent.qnetwork_local, dro_agent.qnetwork_target, dro_agent.tau)\n",
    "\n",
    "        if all(dones):\n",
    "            break\n",
    "        \n",
    "    for i in range(n_envs):\n",
    "        ep_ret = float(episode_scores[i])\n",
    "        train_returns[i].append(ep_ret)\n",
    "        scores_window[i].append(ep_ret)\n",
    "        train_mean_returns[i].append(np.mean(scores_window[i]))\n",
    "\n",
    "    eps = max(eps_end, eps_decay * eps)\n",
    "    env_stats = \" | \".join(\n",
    "        f\"Env{i}: {np.mean(scores_window[i]):.2f}\"\n",
    "        for i in range(n_envs)\n",
    "    )\n",
    "    print(\n",
    "        f\"\\rEpisode {i_episode}\"\n",
    "        f\"\\t{env_stats}\"\n",
    "        f\"\\tEps: {eps:.3f}\",\n",
    "        end=\"\"\n",
    "    )\n",
    "\n",
    "    if i_episode % 10 == 0:\n",
    "        for i, env in enumerate(envs):\n",
    "            eval_list = []\n",
    "            for _ in range(5):\n",
    "                state, _ = env.reset()\n",
    "                hidden = dro_agent.qnetwork_local.init_hidden(1)\n",
    "                total = 0.0\n",
    "\n",
    "                for t in range(max_t):\n",
    "                    action, next_hidden = dro_agent.act(state, eps=0.0, hidden=hidden)\n",
    "                    next_state, reward, done, truncated, _ = env.step(action)\n",
    "                    state = next_state\n",
    "                    hidden = next_hidden\n",
    "                    total += reward\n",
    "                    if done or truncated:\n",
    "                        break\n",
    "                eval_list.append(total)\n",
    "            env_eval_returns[i].append(float(np.mean(eval_list)))\n",
    "            \n",
    "        for i, env in enumerate(test_envs):\n",
    "            eval_list = []\n",
    "            for _ in range(5):\n",
    "                state, _ = env.reset()\n",
    "                hidden = dro_agent.qnetwork_local.init_hidden(1)\n",
    "                total = 0\n",
    "                for t in range(max_t):\n",
    "                    action, next_hidden = dro_agent.act(\n",
    "                        state, eps=0.0, hidden=hidden\n",
    "                    )\n",
    "                    next_state, reward, done, truncated, _ = env.step(action)\n",
    "                    state = next_state\n",
    "                    hidden = next_hidden\n",
    "                    total += reward\n",
    "                    if done or truncated:\n",
    "                        break\n",
    "                eval_list.append(total)\n",
    "            test_eval_returns[i].append(np.mean(eval_list))\n",
    "    # --- clean newline every 100 eps ---\n",
    "    if i_episode % 100 == 0:\n",
    "        print(\"\\n\" + \"-\" * 90)\n",
    "        print(f\"Episode {i_episode}\")\n",
    "        for i in range(n_envs):\n",
    "            line = f\"  Env{i}: Train Avg(100)={np.mean(scores_window[i]):.2f}\"\n",
    "            if env_eval_returns[i]:\n",
    "                line += f\" | Eval={env_eval_returns[i][-1]:.2f}\"\n",
    "            print(line)\n",
    "\n",
    "        for i in range(n_test_envs):\n",
    "            print(f\"  TestEnv{i}: Test Eval={test_eval_returns[i][-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbabc34-6920-41db-bcb8-30d760c95a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = len(envs)\n",
    "n_test_envs = len(test_envs)\n",
    "\n",
    "eval_every = 10  # you evaluate every 10 episodes\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "# --- Training scores ---\n",
    "for i in range(n_envs):\n",
    "    x = np.arange(1, len(train_mean_returns[i]) + 1)\n",
    "    ax1.plot(x, train_mean_returns[i], label=f'Env-{i}')\n",
    "    \n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_xlabel('Episode #')\n",
    "ax1.set_title('Training Returns (Averaged)')\n",
    "ax1.grid(True)\n",
    "ax1.legend(ncol=1)\n",
    "\n",
    "for i in range(n_envs):\n",
    "    eval_x = np.arange(1, len(env_eval_returns[i]) + 1) * eval_every\n",
    "    ax2.plot(\n",
    "        eval_x,\n",
    "        env_eval_returns[i],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        label=f'Env-{i}'\n",
    "    )\n",
    "\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_xlabel('Episode #')\n",
    "ax2.set_title(f'Evaluation on training envs (eps=0, every {eval_every} episodes)')\n",
    "ax2.grid(True)\n",
    "ax2.legend(ncol=1)\n",
    "\n",
    "for i in range(n_test_envs):\n",
    "    eval_test_x = np.arange(1, len(test_eval_returns[i]) + 1) * eval_every\n",
    "    ax3.plot(\n",
    "        eval_test_x,\n",
    "        test_eval_returns[i],\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        label=f'Test-Env-{i}'\n",
    "    )\n",
    "\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_xlabel('Episode #')\n",
    "ax3.set_title(f'Evaluation on test envs (eps=0, every {eval_every} episodes)')\n",
    "ax3.grid(True)\n",
    "ax3.legend(ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0756ec7-616b-41ac-a6f8-79f30fee2d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We want weights to increase for the higher loss\n",
    "#             with torch.no_grad():\n",
    "#                 losses = torch.tensor([loss_1.item(), loss_2.item()])\n",
    "#                 # Update weights: w_i = w_i * exp(eta * loss_i)\n",
    "#                 group_weights = group_weights * torch.exp(DRO_STEP_SIZE * losses)\n",
    "#                 # Normalize so they sum to 1\n",
    "#                 group_weights = group_weights / group_weights.sum()\n",
    "\n",
    "#             # 4. Compute Weighted Robust Loss\n",
    "#             # Weighted sum based on adversarial weights\n",
    "#             robust_loss = group_weights[0] * loss_1 + group_weights[1] * loss_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
